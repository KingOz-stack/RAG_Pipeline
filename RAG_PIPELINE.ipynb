{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KingOz-stack/RAG_Pipeline/blob/main/RAG_PIPELINE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**AI-Driven Document Classification & Retrieval Notebook Using Open Source Model (Mistra)**\n",
        "\n",
        "This notebook showcases an AI-powered system for extracting, classifying, and querying documents. It efficiently processes PDFs, categorizes them into predefined classes, and builds vector indexes for fast and accurate semantic search. Using retrieval-augmented generation (RAG), the system routes user queries to the most relevant documents, delivering precise and context-aware responses.\n"
      ],
      "metadata": {
        "id": "LaozCbUX3wd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Set Up"
      ],
      "metadata": {
        "id": "tD3A5qizGNlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries with CUDA support\n",
        "!pip install -q torch \\\n",
        "    llama-cpp-python==0.2.90 --no-cache-dir --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu123 \\\n",
        "    pymupdf \\\n",
        "    llama-index-llms-llama-cpp \\\n",
        "    llama-index-embeddings-huggingface"
      ],
      "metadata": {
        "id": "LT0xRkzw8jj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99f49aae-860b-4e18-f003-e65c9538fa00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m444.5/444.5 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m159.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m209.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m200.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m243.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m248.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m176.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m206.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m222.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m248.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m250.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m267.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m222.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m250.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m292.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.2/129.2 kB\u001b[0m \u001b[31m273.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# Check CUDA availability and version\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtUU162G82Ec",
        "outputId": "07ded551-e12e-4464-a006-1ca808fc964b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model path\n",
        "model_path = \"/content/mistral.gguf\"\n",
        "\n",
        "# Download Mistral model if not already present\n",
        "if not os.path.exists(model_path):\n",
        "    !wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf \\\n",
        "        -O {model_path}\n",
        "    print(f\"Model downloaded to {model_path}\")\n",
        "\n",
        "# Verify file existence and size\n",
        "if os.path.exists(model_path):\n",
        "    print(f\"Model file exists. Size: {os.path.getsize(model_path) / (1024 * 1024):.2f} MB\")\n",
        "else:\n",
        "    print(\"Model file not found!\")\n",
        "\n",
        "# Load the model with GPU acceleration\n",
        "try:\n",
        "    llm = Llama(\n",
        "        model_path=model_path,\n",
        "        n_gpu_layers=1,  # Start with 1 layer on GPU to be safe\n",
        "        n_ctx=2048,      # Context window size\n",
        "        verbose=True     # Show loading progress\n",
        "    )\n",
        "    print(\"Model loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVynXaeB8-Rd",
        "outputId": "88895cbe-9094-4080-e77e-4acaa86b5666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-02 23:04:05--  https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.169.137.5, 3.169.137.119, 3.169.137.111, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.169.137.5|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/72/62/726219e98582d16c24a66629a4dec1b0761b91c918e15dea2625b4293c134a92/3e0039fd0273fcbebb49228943b17831aadd55cbcbf56f0af00499be2040ccf9?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.2.Q4_K_M.gguf%3B+filename%3D%22mistral-7b-instruct-v0.2.Q4_K_M.gguf%22%3B&Expires=1743638645&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzYzODY0NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzcyLzYyLzcyNjIxOWU5ODU4MmQxNmMyNGE2NjYyOWE0ZGVjMWIwNzYxYjkxYzkxOGUxNWRlYTI2MjViNDI5M2MxMzRhOTIvM2UwMDM5ZmQwMjczZmNiZWJiNDkyMjg5NDNiMTc4MzFhYWRkNTVjYmNiZjU2ZjBhZjAwNDk5YmUyMDQwY2NmOT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=m1VHwOv4YM1U3KWv5Rv1jm1S8dpJXZR3UnDeIlfi%7E48K2OxzO1f67C6bPYjUMJ0Tuwt8aRi78FcbGKQ7djRCyQiXOw9BjFbXtcbCqnUpXjMc5HZgQAxSKVXHYDlR5aNzyy0aPp11WiG8ryC-rBi8Sy4mC4aXqhwXsbK%7EAHEZHP9v95F7DeBSbpcAXt4aqaLDVnr5KGlxfkH-BQ5odGLaawkSFMmYEwfakdAyW3PyMJ54j9QFe%7E2c6pQUTKWuL0J2kRjMM4BREJR5nTTQ4DnQQjzwTKFYcHybE%7Exd63%7EMpZ9qwXF7kI4a3ZKayGIG-kJFBIT7AKU2jHxTNDHAUUCzNA__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-04-02 23:04:05--  https://cdn-lfs-us-1.hf.co/repos/72/62/726219e98582d16c24a66629a4dec1b0761b91c918e15dea2625b4293c134a92/3e0039fd0273fcbebb49228943b17831aadd55cbcbf56f0af00499be2040ccf9?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.2.Q4_K_M.gguf%3B+filename%3D%22mistral-7b-instruct-v0.2.Q4_K_M.gguf%22%3B&Expires=1743638645&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzYzODY0NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzcyLzYyLzcyNjIxOWU5ODU4MmQxNmMyNGE2NjYyOWE0ZGVjMWIwNzYxYjkxYzkxOGUxNWRlYTI2MjViNDI5M2MxMzRhOTIvM2UwMDM5ZmQwMjczZmNiZWJiNDkyMjg5NDNiMTc4MzFhYWRkNTVjYmNiZjU2ZjBhZjAwNDk5YmUyMDQwY2NmOT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=m1VHwOv4YM1U3KWv5Rv1jm1S8dpJXZR3UnDeIlfi%7E48K2OxzO1f67C6bPYjUMJ0Tuwt8aRi78FcbGKQ7djRCyQiXOw9BjFbXtcbCqnUpXjMc5HZgQAxSKVXHYDlR5aNzyy0aPp11WiG8ryC-rBi8Sy4mC4aXqhwXsbK%7EAHEZHP9v95F7DeBSbpcAXt4aqaLDVnr5KGlxfkH-BQ5odGLaawkSFMmYEwfakdAyW3PyMJ54j9QFe%7E2c6pQUTKWuL0J2kRjMM4BREJR5nTTQ4DnQQjzwTKFYcHybE%7Exd63%7EMpZ9qwXF7kI4a3ZKayGIG-kJFBIT7AKU2jHxTNDHAUUCzNA__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.169.36.31, 3.169.36.38, 3.169.36.120, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.169.36.31|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4368439584 (4.1G) [binary/octet-stream]\n",
            "Saving to: â€˜/content/mistral.ggufâ€™\n",
            "\n",
            "/content/mistral.gg 100%[===================>]   4.07G   191MB/s    in 22s     \n",
            "\n",
            "2025-04-02 23:04:27 (188 MB/s) - â€˜/content/mistral.ggufâ€™ saved [4368439584/4368439584]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /content/mistral.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens cache size = 3\n",
            "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model downloaded to /content/mistral.gguf\n",
            "Model file exists. Size: 4166.07 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
            "llm_load_tensors: offloading 1 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 1/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =   132.50 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 2048\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 1000000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:  CUDA_Host KV buffer size =   248.00 MiB\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =     8.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   181.04 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    12.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Guessed chat format: mistral-instruct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Required Libraries**"
      ],
      "metadata": {
        "id": "z--RjLe9dNYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "from llama_index.core import VectorStoreIndex, Document, Settings\n",
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.response_synthesizers import CompactAndRefine"
      ],
      "metadata": {
        "id": "ume-fKZYdeVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `fitz `(PyMuPDF) is used for extracting text from PDF documents.\n",
        "\n",
        "* `re `is the regular expressions module for text processing.\n",
        "\n",
        "* `llama_index.core` provides components for document indexing, retrieval, and querying.\n",
        "\n",
        "* `LlamaCPP `is used to integrate an open-source LLM for classification and query processing.\n",
        "\n",
        "* `HuggingFaceEmbedding `is used for embedding document texts for vector search.\n",
        "\n",
        "* `RetrieverQueryEngine` and `CompactAndRefine` help process queries and synthesize responses.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2skAijHwd6VQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DocumentClassifier Class Initialization**"
      ],
      "metadata": {
        "id": "21ZdWDH-fHty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentClassifier:\n",
        "    def __init__(self, model_path, embedding_model=\"BAAI/bge-small-en-v1.5\"):"
      ],
      "metadata": {
        "id": "4xrmZsrifRQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class initializes an LLM-based document classifier.\n",
        "\n",
        "`model_path:` Specifies the path to the LLM model (e.g., Mistral).\n",
        "\n",
        "`embedding_model: `Loads a Hugging Face model for text embeddings to perform similarity searches."
      ],
      "metadata": {
        "id": "JPEOt0HRfYx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "self.llm = LlamaCPP(\n",
        "    model_path=model_path,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=50,\n",
        "    context_window=4096\n",
        ")"
      ],
      "metadata": {
        "id": "lWpETbYWfqlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Loads the Mistral LLM with specific parameters:\n",
        "\n",
        "* temperature=0.1 keeps responses consistent.\n",
        "\n",
        "* max_new_tokens=50 controls response length.\n",
        "\n",
        "* context_window=4096 ensures enough memory for processing long text."
      ],
      "metadata": {
        "id": "4hTgLcOtfvqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "self.embed_model = HuggingFaceEmbedding(model_name=embedding_model)"
      ],
      "metadata": {
        "id": "pzbxRXEMgD4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Loads a transformer-based embedding model for document indexing and retrieval."
      ],
      "metadata": {
        "id": "tx_qsnbNgJB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "self.DOCUMENT_CATEGORIES = [\n",
        "    'Bank Statement',\n",
        "    'Pay Slip',\n",
        "    'Appraisal Report',\n",
        "    'Lender Fees Worksheet',\n",
        "    'Sample Contract'\n",
        "]"
      ],
      "metadata": {
        "id": "gLtktfPXgsH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Defines document categories that the classifier will identify."
      ],
      "metadata": {
        "id": "OhNhwJRngzBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "self.index_map = {}"
      ],
      "metadata": {
        "id": "eiYgI2EzhOl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A dictionary to store vector indexes for each classified document type."
      ],
      "metadata": {
        "id": "kMHINX19hQEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Extracting Text from PDFs**"
      ],
      "metadata": {
        "id": "1nUxsCPWhujh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_pdf_text(self, pdf_path):\n",
        "    \"\"\"Extract text from a given PDF file.\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        text = \"\\n\".join([page.get_text(\"text\", sort=True) for page in doc])\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "smEHyijkkp9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Opens a PDF and extracts text from all pages.\n",
        "\n",
        "* Returns extracted text as a single string.\n",
        "\n",
        "* If an error occurs, it prints a message instead of breaking."
      ],
      "metadata": {
        "id": "KHaeZLlikshZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Preprocessing Text for Classification**"
      ],
      "metadata": {
        "id": "WTLEohwvlZXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_document_for_classification(self, text):\n",
        "    \"\"\"Create a structured representation of document text.\"\"\""
      ],
      "metadata": {
        "id": "d-ahke7Kl3zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracts important portions of a document for classification."
      ],
      "metadata": {
        "id": "a2Br4eURl9vC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_length = len(text)\n",
        "first_part = text[:min(500, doc_length)]\n",
        "middle_start = max(0, doc_length // 2 - 250)\n",
        "middle_part = text[middle_start:middle_start + min(500, doc_length - middle_start)]\n",
        "last_part = text[-500:] if doc_length > 500 else text"
      ],
      "metadata": {
        "id": "Uu6cQxnvmoDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Extracts three key sections: beginning, middle, and end.\n",
        "\n",
        "* Helps classification by focusing on meaningful parts of a document."
      ],
      "metadata": {
        "id": "yPP9BkT4mtPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "potential_headers = [\n",
        "    line.strip() for line in text.split('\\n')\n",
        "    if line.strip() and len(line.strip()) < 50 and line.strip().isupper()\n",
        "][:5]"
      ],
      "metadata": {
        "id": "lsbQUALCnEtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Identifies potential document headers (titles, section names) for better classification.\n"
      ],
      "metadata": {
        "id": "S0ET4fFKnMKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Classifying Documents**"
      ],
      "metadata": {
        "id": "fyaANXK1nwtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_document(self, text):\n",
        "    \"\"\"Classify document into predefined categories.\"\"\""
      ],
      "metadata": {
        "id": "z5Tr_01Dn5G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Uses LLM to classify a document into one of the predefined categories"
      ],
      "metadata": {
        "id": "ikSU7LIXn_SP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"Classify this document into one of these precise categories:\n",
        "...\n",
        "ONLY respond with the EXACT category name:\"\"\""
      ],
      "metadata": {
        "id": "V0Isy9QxoZZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Provides an LLM prompt that describes each category and asks for a classification."
      ],
      "metadata": {
        "id": "gpi1fQn_ofwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = self.llm.complete(prompt)\n",
        "classified_type = response.text.strip()"
      ],
      "metadata": {
        "id": "e5SyG1cXowEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Calls the LLM to generate a classification."
      ],
      "metadata": {
        "id": "jgYaVHr0oxkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for category in self.DOCUMENT_CATEGORIES:\n",
        "    if category.lower() in classified_type.lower():\n",
        "        return category"
      ],
      "metadata": {
        "id": "7UE3PGdUpZhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Performs fuzzy matching to ensure classification aligns with predefined categories.\n"
      ],
      "metadata": {
        "id": "gkxYZa2UpbKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Building Indexes for Classified Documents**"
      ],
      "metadata": {
        "id": "i0yFI8WjqWK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_indexes(self, document_paths):\n",
        "    \"\"\"Builds vector indexes for classified documents.\"\"\""
      ],
      "metadata": {
        "id": "ImcVPrqmqaY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Loops through each document, extracts text, classifies it, and stores it in a vector index."
      ],
      "metadata": {
        "id": "mQ6U66HaqgC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if doc_type != \"Unknown\":\n",
        "    document = Document(text=text, metadata={\"doc_type\": doc_type})\n",
        "\n",
        "    if doc_type not in self.index_map:\n",
        "        self.index_map[doc_type] = VectorStoreIndex.from_documents(\n",
        "            [document], embed_model=self.embed_model\n",
        "        )\n",
        "    else:\n",
        "        self.index_map[doc_type].insert(document)"
      ],
      "metadata": {
        "id": "YIIWPwEuqotZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Stores classified documents in vector databases for retrieval.\n"
      ],
      "metadata": {
        "id": "qSJPeoocqs98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Weighted Retrieval Approach**"
      ],
      "metadata": {
        "id": "Ui7CE5jtryVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def route_query(self, query):\n",
        "    \"\"\"Routes a query to the relevant document type and retrieves an answer.\"\"\"\n",
        "\n",
        "    type_prompt = f\"\"\"Classify this query to the most relevant document type:\n",
        "...\n",
        "Respond ONLY with: Bank Statement, Pay Slip, Appraisal Report, Lender Fees Worksheet, Sample Contract, or Unknown\"\"\"\n",
        "\n",
        "# Uses LLM to decide which document type is most relevant.\n",
        "\n",
        "doc_type_response = self.llm.complete(type_prompt)\n",
        "doc_type = doc_type_response.text.strip()\n",
        "\n",
        "# Calls the LLM to classify the query.\n",
        "\n",
        "if doc_type not in self.index_map:\n",
        "    return f\"Unable to route query to document type: {doc_type}\"\n",
        "\n",
        "if doc_type not in self.index_map:\n",
        "    return f\"Unable to route query to document type: {doc_type}\"\n",
        "\n",
        "#If the classified document type isnâ€™t indexed, the query can't be answered.\n",
        "\n",
        "\n",
        "retriever = self.index_map[doc_type].as_retriever(similarity_top_k=2)\n",
        "response_synthesizer = CompactAndRefine(llm=self.llm)\n",
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever,\n",
        "    response_synthesizer=response_synthesizer\n",
        ")\n",
        "\n",
        "#* Implements hybrid retrieval:\n",
        "#* Uses similarity-based document retrieval.\n",
        "#* Synthesizes a refined answer using LLM.\n",
        "\n",
        "response = query_engine.query(query)\n",
        "return f\"ðŸ“„ **Document Type:** {doc_type}\\nðŸ” **Answer:** {response}\"\n",
        "#Retrieves an answer from the most relevant document type."
      ],
      "metadata": {
        "id": "IYJ-wivQr1kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8.  Running the Classifier and Query System**"
      ],
      "metadata": {
        "id": "JRlF5uG-smcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    document_paths = [\n",
        "        \"/content/payslip_sample_image.pdf\",\n",
        "        \"/content/sample_bank_statement.pdf\",\n",
        "        \"/content/appraisal_report.pdf\",\n",
        "        \"/content/LenderFeesWorksheetNew.pdf\",\n",
        "        \"/content/sample_contract.pdf\"\n",
        "    ]\n",
        "#Lists the paths to documents that need classification.\n",
        "\n",
        "model_path = \"/content/mistral.gguf\"\n",
        "classifier = DocumentClassifier(model_path)\n",
        "classifier.build_indexes(document_paths)\n",
        "\n",
        "#Loads the Mistral model and builds document indexes\n",
        "\n",
        "test_queries = [\n",
        "    \"What is my net salary?\",\n",
        "    \"What is the appraised value of the house?\",\n",
        "    \"What was my last deposit?\"\n",
        "]\n",
        "\n",
        "#Example queries to test document classification and retrieval.\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    print(classifier.route_query(query))\n",
        "#Runs each query through the classifier and prints the response.\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "#Ensures the script runs when executed."
      ],
      "metadata": {
        "id": "V59J-CURtDCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RAG PIPELINE IS BELOW:**"
      ],
      "metadata": {
        "id": "0w2-JQAbtxaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "from llama_index.core import VectorStoreIndex, Document, Settings\n",
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.response_synthesizers import CompactAndRefine\n",
        "\n",
        "class DocumentClassifier:\n",
        "    def __init__(self, model_path, embedding_model=\"BAAI/bge-small-en-v1.5\"):\n",
        "        # Initialize LLM with optimized parameters\n",
        "        self.llm = LlamaCPP(\n",
        "            model_path=model_path,\n",
        "            temperature=0.1,  # Slightly lower temperature for more consistent results\n",
        "            max_new_tokens=50,  # Increased to allow more flexibility\n",
        "            context_window=4096\n",
        "        )\n",
        "\n",
        "        # Initialize embedding model\n",
        "        self.embed_model = HuggingFaceEmbedding(model_name=embedding_model)\n",
        "\n",
        "        # Predefined document categories\n",
        "        self.DOCUMENT_CATEGORIES = [\n",
        "            'Bank Statement',\n",
        "            'Pay Slip',\n",
        "            'Appraisal Report',\n",
        "            'Lender Fees Worksheet',\n",
        "            'Sample Contract'\n",
        "        ]\n",
        "\n",
        "        # Index map to store document indexes\n",
        "        self.index_map = {}\n",
        "\n",
        "    def extract_pdf_text(self, pdf_path):\n",
        "        \"\"\"Extract text from PDF with improved text extraction.\"\"\"\n",
        "        try:\n",
        "            doc = fitz.open(pdf_path)\n",
        "            text = \"\\n\".join([page.get_text(\"text\", sort=True) for page in doc])\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text from {pdf_path}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def prepare_document_for_classification(self, text):\n",
        "        \"\"\"Create a more comprehensive document representation.\"\"\"\n",
        "        doc_length = len(text)\n",
        "\n",
        "        # Extract key sections\n",
        "        first_part = text[:min(500, doc_length)]\n",
        "        middle_start = max(0, doc_length // 2 - 250)\n",
        "        middle_part = text[middle_start:middle_start + min(500, doc_length - middle_start)]\n",
        "        last_part = text[-500:] if doc_length > 500 else text\n",
        "\n",
        "        # Extract potential headers or key phrases\n",
        "        potential_headers = [\n",
        "            line.strip() for line in text.split('\\n')\n",
        "            if line.strip() and len(line.strip()) < 50 and line.strip().isupper()\n",
        "        ][:5]\n",
        "\n",
        "        return {\n",
        "            \"first_part\": first_part,\n",
        "            \"middle_part\": middle_part,\n",
        "            \"last_part\": last_part,\n",
        "            \"total_length\": doc_length,\n",
        "            \"potential_headers\": \"\\n\".join(potential_headers) if potential_headers else \"\"\n",
        "        }\n",
        "\n",
        "    def classify_document(self, text):\n",
        "        \"\"\"Classify document with improved prompting and fallback mechanisms.\"\"\"\n",
        "        doc_info = self.prepare_document_for_classification(text)\n",
        "\n",
        "        prompt = f\"\"\"Classify this document into one of these precise categories:\n",
        "        - Bank Statement: Official financial record showing account transactions\n",
        "        - Pay Slip: Employment earnings document with salary details\n",
        "        - Appraisal Report: Professional property valuation document\n",
        "        - Lender Fees Worksheet: Detailed loan cost breakdown\n",
        "        - Sample Contract: Legal document template or example\n",
        "        - Unknown: If no clear match exists\n",
        "\n",
        "        Extracted Document Characteristics:\n",
        "        First Excerpt: {doc_info['first_part']}\n",
        "        Middle Excerpt: {doc_info['middle_part']}\n",
        "        End Excerpt: {doc_info['last_part']}\n",
        "        Potential Headers: {doc_info['potential_headers']}\n",
        "        Total Length: {doc_info['total_length']} characters\n",
        "\n",
        "        ONLY respond with the EXACT category name:\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.llm.complete(prompt)\n",
        "            classified_type = response.text.strip()\n",
        "\n",
        "            # Fuzzy matching for categories\n",
        "            for category in self.DOCUMENT_CATEGORIES:\n",
        "                if category.lower() in classified_type.lower():\n",
        "                    return category\n",
        "\n",
        "            return \"Unknown\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Classification error: {e}\")\n",
        "            return \"Unknown\"\n",
        "\n",
        "    def build_indexes(self, document_paths):\n",
        "        \"\"\"Build vector indexes for classified documents.\"\"\"\n",
        "        classified_docs = {}\n",
        "\n",
        "        for doc_path in document_paths:\n",
        "            text = self.extract_pdf_text(doc_path)\n",
        "            doc_type = self.classify_document(text)\n",
        "\n",
        "            if doc_type != \"Unknown\":\n",
        "                document = Document(text=text, metadata={\"doc_type\": doc_type})\n",
        "\n",
        "                if doc_type not in self.index_map:\n",
        "                    self.index_map[doc_type] = VectorStoreIndex.from_documents(\n",
        "                        [document],\n",
        "                        embed_model=self.embed_model\n",
        "                    )\n",
        "                else:\n",
        "                    self.index_map[doc_type].insert(document)\n",
        "\n",
        "        return self.index_map\n",
        "\n",
        "    def route_query(self, query):\n",
        "        \"\"\"Route query to appropriate document type and retrieve answer.\"\"\"\n",
        "        # Use LLM to determine document type\n",
        "        type_prompt = f\"\"\"Classify this query to the most relevant document type:\n",
        "        Query: {query}\n",
        "        Respond ONLY with: Bank Statement, Pay Slip, Appraisal Report, Lender Fees Worksheet, Sample Contract, or Unknown\"\"\"\n",
        "\n",
        "        try:\n",
        "            doc_type_response = self.llm.complete(type_prompt)\n",
        "            doc_type = doc_type_response.text.strip()\n",
        "\n",
        "            # Fuzzy matching\n",
        "            for category in self.DOCUMENT_CATEGORIES:\n",
        "                if category.lower() in doc_type.lower():\n",
        "                    doc_type = category\n",
        "                    break\n",
        "\n",
        "            # Check if document type exists in our index\n",
        "            if doc_type not in self.index_map:\n",
        "                return f\"Unable to route query to document type: {doc_type}\"\n",
        "\n",
        "            # Create query engine\n",
        "            retriever = self.index_map[doc_type].as_retriever(similarity_top_k=2)\n",
        "            response_synthesizer = CompactAndRefine(llm=self.llm)\n",
        "            query_engine = RetrieverQueryEngine(\n",
        "                retriever=retriever,\n",
        "                response_synthesizer=response_synthesizer\n",
        "            )\n",
        "\n",
        "            # Execute query\n",
        "            response = query_engine.query(query)\n",
        "            return f\"Document Type: {doc_type}\\n Answer: {response}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Query routing error: {e}\"\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    # Paths to your documents\n",
        "    document_paths = [\n",
        "        \"/content/payslip_sample_image.pdf\",\n",
        "        \"/content/sample_bank_statement.pdf\",\n",
        "        \"/content/appraisal_report.pdf\",\n",
        "        \"/content/LenderFeesWorksheetNew.pdf\",\n",
        "        \"/content/sample_contract.pdf\"\n",
        "    ]\n",
        "\n",
        "    # Path to your Mistral model\n",
        "    model_path = \"/content/mistral.gguf\"\n",
        "\n",
        "    # Initialize classifier\n",
        "    classifier = DocumentClassifier(model_path)\n",
        "\n",
        "    # Build indexes\n",
        "    classifier.build_indexes(document_paths)\n",
        "\n",
        "    # Test queries\n",
        "    test_queries = [\n",
        "        \"What is my net salary?\",\n",
        "        \"What is the appraised value of the house?\",\n",
        "        \"What was my last deposit?\"\n",
        "        \"How much tax was deducted from the paycheck?\"\n",
        "    ]\n",
        "\n",
        "    for query in test_queries:\n",
        "        print(f\"\\nQuery: {query}\")\n",
        "        print(classifier.route_query(query))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EgInRbBGmYf",
        "outputId": "11bd4f4c-641b-4d1b-98c3-fd21c865e26d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /content/mistral.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens cache size = 3\n",
            "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
            "llm_load_tensors: offloading 0 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 0/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 1000000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:  CUDA_Host KV buffer size =   512.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   297.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    16.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 356\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Guessed chat format: mistral-instruct\n",
            "\n",
            "llama_print_timings:        load time =    2456.24 ms\n",
            "llama_print_timings:      sample time =       0.22 ms /     4 runs   (    0.05 ms per token, 18433.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2455.36 ms /   478 tokens (    5.14 ms per token,   194.68 tokens per second)\n",
            "llama_print_timings:        eval time =    2076.07 ms /     3 runs   (  692.02 ms per token,     1.45 tokens per second)\n",
            "llama_print_timings:       total time =    4534.45 ms /   481 tokens\n",
            "Llama.generate: 107 prefix-match hit, remaining 529 prompt tokens to eval\n",
            "\n",
            "llama_print_timings:        load time =    2456.24 ms\n",
            "llama_print_timings:      sample time =       0.18 ms /     4 runs   (    0.05 ms per token, 21739.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7340.86 ms /   529 tokens (   13.88 ms per token,    72.06 tokens per second)\n",
            "llama_print_timings:        eval time =    1796.12 ms /     3 runs   (  598.71 ms per token,     1.67 tokens per second)\n",
            "llama_print_timings:       total time =    9141.25 ms /   532 tokens\n",
            "Llama.generate: 108 prefix-match hit, remaining 386 prompt tokens to eval\n",
            "\n",
            "llama_print_timings:        load time =    2456.24 ms\n",
            "llama_print_timings:      sample time =       0.25 ms /     5 runs   (    0.05 ms per token, 20000.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2359.26 ms /   386 tokens (    6.11 ms per token,   163.61 tokens per second)\n",
            "llama_print_timings:        eval time =    2393.59 ms /     4 runs   (  598.40 ms per token,     1.67 tokens per second)\n",
            "llama_print_timings:       total time =    4757.07 ms /   390 tokens\n",
            "Llama.generate: 105 prefix-match hit, remaining 421 prompt tokens to eval\n",
            "\n",
            "llama_print_timings:        load time =    2456.24 ms\n",
            "llama_print_timings:      sample time =       0.42 ms /     8 runs   (    0.05 ms per token, 18823.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1449.46 ms /   421 tokens (    3.44 ms per token,   290.45 tokens per second)\n",
            "llama_print_timings:        eval time =    4170.88 ms /     7 runs   (  595.84 ms per token,     1.68 tokens per second)\n",
            "llama_print_timings:       total time =    5626.56 ms /   428 tokens\n",
            "Llama.generate: 105 prefix-match hit, remaining 479 prompt tokens to eval\n",
            "\n",
            "llama_print_timings:        load time =    2456.24 ms\n",
            "llama_print_timings:      sample time =       0.93 ms /    16 runs   (    0.06 ms per token, 17130.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2572.09 ms /   479 tokens (    5.37 ms per token,   186.23 tokens per second)\n",
            "llama_print_timings:        eval time =    9012.73 ms /    15 runs   (  600.85 ms per token,     1.66 tokens per second)\n",
            "llama_print_timings:       total time =   11596.82 ms /   494 tokens\n",
            "Llama.generate: 4 prefix-match hit, remaining 51 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: What is my net salary?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =    2456.24 ms\n",
            "llama_print_timings:      sample time =       0.60 ms /    10 runs   (    0.06 ms per token, 16528.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1105.21 ms /    51 tokens (   21.67 ms per token,    46.14 tokens per second)\n",
            "llama_print_timings:        eval time =    5652.63 ms /     9 runs   (  628.07 ms per token,     1.59 tokens per second)\n",
            "llama_print_timings:       total time =    6766.53 ms /    60 tokens\n",
            "Llama.generate: 1 prefix-match hit, remaining 329 prompt tokens to eval\n",
            "\n",
            "llama_print_timings:        load time =    2456.24 ms\n",
            "llama_print_timings:      sample time =       0.30 ms /     6 runs   (    0.05 ms per token, 19933.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2523.99 ms /   329 tokens (    7.67 ms per token,   130.35 tokens per second)\n",
            "llama_print_timings:        eval time =    2916.91 ms /     5 runs   (  583.38 ms per token,     1.71 tokens per second)\n",
            "llama_print_timings:       total time =    5445.66 ms /   334 tokens\n",
            "Llama.generate: 1 prefix-match hit, remaining 59 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Type: Pay Slip\n",
            " Answer: 9500.\n",
            "\n",
            "Query: What is the appraised value of the house?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =    2456.24 ms\n",
            "llama_print_timings:      sample time =       0.64 ms /    12 runs   (    0.05 ms per token, 18779.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1106.82 ms /    59 tokens (   18.76 ms per token,    53.31 tokens per second)\n",
            "llama_print_timings:        eval time =    6895.72 ms /    11 runs   (  626.88 ms per token,     1.60 tokens per second)\n",
            "llama_print_timings:       total time =    8010.22 ms /    70 tokens\n",
            "Llama.generate: 1 prefix-match hit, remaining 2330 prompt tokens to eval\n",
            "\n",
            "llama_print_timings:        load time =    2456.24 ms\n",
            "llama_print_timings:      sample time =       1.18 ms /    21 runs   (    0.06 ms per token, 17872.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8593.07 ms /  2330 tokens (    3.69 ms per token,   271.15 tokens per second)\n",
            "llama_print_timings:        eval time =   14152.41 ms /    20 runs   (  707.62 ms per token,     1.41 tokens per second)\n",
            "llama_print_timings:       total time =   22767.62 ms /  2350 tokens\n",
            "Llama.generate: 1 prefix-match hit, remaining 54 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Type: Appraisal Report\n",
            " Answer:  The appraised value of the house is $1,918,507.\n",
            "\n",
            "Query: What was my last deposit?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =    2456.24 ms\n",
            "llama_print_timings:      sample time =       0.47 ms /     9 runs   (    0.05 ms per token, 19027.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1079.55 ms /    54 tokens (   19.99 ms per token,    50.02 tokens per second)\n",
            "llama_print_timings:        eval time =    4553.34 ms /     8 runs   (  569.17 ms per token,     1.76 tokens per second)\n",
            "llama_print_timings:       total time =    5638.29 ms /    62 tokens\n",
            "Llama.generate: 1 prefix-match hit, remaining 1414 prompt tokens to eval\n",
            "\n",
            "llama_print_timings:        load time =    2456.24 ms\n",
            "llama_print_timings:      sample time =       1.45 ms /    31 runs   (    0.05 ms per token, 21453.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5569.21 ms /  1414 tokens (    3.94 ms per token,   253.90 tokens per second)\n",
            "llama_print_timings:        eval time =   19563.20 ms /    30 runs   (  652.11 ms per token,     1.53 tokens per second)\n",
            "llama_print_timings:       total time =   25155.49 ms /  1444 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Type: Bank Statement\n",
            " Answer:  Your last deposit was $2,678.39 on July 31, 2018, via an ATM.\n"
          ]
        }
      ]
    }
  ]
}